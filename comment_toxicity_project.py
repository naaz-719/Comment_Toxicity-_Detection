# -*- coding: utf-8 -*-
"""Comment_Toxicity_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VkKI8rSf0Z3kp_Y8R95sNDalnRMwFPx8

# **STEP 1: DATA EXPLORATION AND PREPARATION**
"""

# Import necessary libraries
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import nltk

!pip install tensorflow pandas scikit-learn nltk

# --- Download NLTK data (required for tokenization and stopwords) ---
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)

# --- Mount Google Drive to access files ---
from google.colab import drive
drive.mount('/content/drive')

# Make sure your project folder is in Google Drive
project_folder = '/content/drive/MyDrive/Comment_Toxicity_Project/Datasets'

# Load the training and testing datasets
try:
    # Assuming 'train.csv' and 'test.csv' are in your project folder
    train_df = pd.read_csv(f'/content/drive/MyDrive/Comment_Toxicity_Project/Datasets/train.csv')
    test_df = pd.read_csv(f'/content/drive/MyDrive/Comment_Toxicity_Project/Datasets/test.csv')
    print("Training and testing datasets loaded successfully!")
except FileNotFoundError:
    print("Error: 'train.csv' or 'test.csv' not found. Please upload the files to your Google Drive.")

# Display basic information about the datasets
print("Train data info:")
print(train_df.info())
print("\nTest data info:")
print(test_df.info())

train_df.shape

# Define the preprocessing function
def preprocess_text(text):
    # 1. Convert text to a string and lowercase it
    text = str(text).lower()

    # 2. Remove special characters, numbers, and punctuation
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # 3. Tokenize the text
    tokens = word_tokenize(text)

    # 4. Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # 5. Join the tokens back into a single string
    return ' '.join(filtered_tokens)

# Apply the preprocessing function to both datasets
train_df['preprocessed_text'] = train_df['comment_text'].apply(preprocess_text)
test_df['preprocessed_text'] = test_df['comment_text'].apply(preprocess_text)

"""# **STEP 2: MODEL DEVELOPMENT**"""

# Define parameters for the tokenizer and sequences
MAX_WORDS = 20000  # Number of unique words to consider
MAX_LEN = 200      # Max length of a sequence

# Initialize and fit the tokenizer on the training text
# It's crucial to fit the tokenizer only on the training data to avoid data leakage
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
tokenizer.fit_on_texts(train_df['preprocessed_text'])

# Convert text to sequences and pad them for both datasets
train_sequences = tokenizer.texts_to_sequences(train_df['preprocessed_text'])
test_sequences = tokenizer.texts_to_sequences(test_df['preprocessed_text'])

X_train = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')
X_test = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')

# Get the labels for both datasets
y_train = train_df['toxic']
# Note: The test dataset does not have toxicity labels, so we cannot get y_test from it.
# y_test = test_df['toxic']

# Build the deep learning model (Simple LSTM)
embedding_dim = 128
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(MAX_WORDS, embedding_dim, input_length=MAX_LEN),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using the pre-split training data
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)

"""# **STEP 3 : SAVING THE MODEL AND TOKENIZER**"""

# --- STEP 3: SAVE THE MODEL AND TOKENIZER ---
model_path = '/content/drive/MyDrive/Comment_Toxicity_Project/toxicity_model.keras'
tokenizer_path = '/content/drive/MyDrive/Comment_Toxicity_Project/tokenizer.pickle'

# Save the trained model in the new Keras format
model.save(model_path)
print(f"Model saved to {model_path}")

# Save the tokenizer (no change needed here)
with open(tokenizer_path, 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
print(f"Tokenizer saved to {tokenizer_path}")